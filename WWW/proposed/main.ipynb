{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import os\n",
    "from huggingface_hub import login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HUGGINGFACE_TOKEN'] = 'your hugging face token'\n",
    "login(os.environ['HUGGINGFACE_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_stats(df_):\n",
    "    df = df_.copy()\n",
    "    \n",
    "    # Plotting the frequency of categories\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(y=df['meme_anxiety_categories'], order=df['meme_anxiety_categories'].value_counts().index)\n",
    "    \n",
    "    plt.title('Frequency of Meme Anxiety Categories')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Categories')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_df =pd.read_csv('path to your main anxiety dataset test csv')\n",
    "train_df =pd.read_csv('path to your main anxiety dataset train csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['meme_anxiety_categories'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['meme_anxiety_categories'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dropna(inplace = True)\n",
    "test_df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns='Unnamed: 0',inplace = True)\n",
    "test_df.drop(columns='Unnamed: 0',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_stats(train_df)\n",
    "show_stats(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_ocr(ocr, ocr_word_limit):\n",
    "        ocr_words = ocr.split()\n",
    "        \n",
    "        if len(ocr_words) > ocr_word_limit:\n",
    "            ocr = ' '.join(ocr_words[:ocr_word_limit])\n",
    "        return ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def detect_formatting(triples):\n",
    "    \"\"\" Detects the formatting used for section titles. \"\"\"\n",
    "    if '###' in triples:\n",
    "        return 'hash'\n",
    "    elif '**' in triples:\n",
    "        return 'star'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "def process_triples(triples):\n",
    "    \"\"\" Process the triples based on the detected formatting. \"\"\"\n",
    "    formatting = detect_formatting(triples)\n",
    "    \n",
    "    # Define the sections you want to keep\n",
    "    sections_to_keep = ['Cause-Effect', 'Figurative Understanding', 'Mental State']\n",
    "    \n",
    "    if formatting == 'hash':\n",
    "        # Normalize and format the text for hash-based sections\n",
    "        normalized_triples = re.sub(r'\\d+\\.\\s*', '', triples)  # Remove numbering\n",
    "        sections = re.split(r'###\\s*(.*)', normalized_triples)\n",
    "    elif formatting == 'star':\n",
    "        # Normalize and format the text for star-based sections\n",
    "        normalized_triples = re.sub(r'\\d+\\.\\s*', '', triples)  # Remove numbering\n",
    "        normalized_triples = re.sub(r'\\*\\*\\s*', '**', normalized_triples)  # Standardize '**'\n",
    "        sections = re.split(r'(\\*\\*.*?\\*\\*)', normalized_triples)\n",
    "    else:\n",
    "        return \"Unknown formatting detected\"\n",
    "    \n",
    "    # Filter and keep only the desired sections\n",
    "    reduced_triples = []\n",
    "    \n",
    "    i = 1\n",
    "    while i < len(sections):\n",
    "        if formatting == 'hash':\n",
    "            section_title = sections[i].strip()\n",
    "            if section_title in sections_to_keep and i + 1 < len(sections):\n",
    "                clean_text = sections[i+1].strip()\n",
    "                reduced_triples.append(f\"{section_title.lower()} : \\n   {clean_text}\")\n",
    "        elif formatting == 'star':\n",
    "            section_title = sections[i].strip(\"**: \").strip()\n",
    "            if section_title in sections_to_keep and i + 1 < len(sections):\n",
    "                clean_text = sections[i+1].strip()\n",
    "                clean_text = re.sub(r'^\\d+\\.\\s*', '', clean_text, flags=re.MULTILINE).strip()\n",
    "                reduced_triples.append(f\"{section_title.lower()} : \\n   {clean_text}\")\n",
    "        i += 2\n",
    "    \n",
    "    # Return the formatted sections\n",
    "    if reduced_triples:\n",
    "        formated_string = '\\n'.join(reduced_triples).strip().replace('-', '').replace(' : \\n   :', ' : ')\n",
    "        return formated_string\n",
    "    else:\n",
    "        return \"No relevant sections found\"\n",
    "\n",
    "# Apply the function to the 'triples' column\n",
    "train_df['triples'] = train_df['triples'].apply(process_triples)\n",
    "test_df['triples'] = test_df['triples'].apply(process_triples)\n",
    "\n",
    "print('------------------------------------------------')\n",
    "print(train_df['triples'][433])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df['triples'][315])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_strings_count = (train_df['triples'] == \"\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_df['labels'] = None\n",
    "test_df['labels'] = None\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_df['labels'] = le.fit_transform(train_df['meme_anxiety_categories'])\n",
    "test_df['labels'] = le.transform(test_df['meme_anxiety_categories'])\n",
    "\n",
    "\n",
    "def prompt_it(df, n_shot=1):\n",
    "    df['prompt'] = None\n",
    "    ocr_word_limit = 75\n",
    "    prompts = []\n",
    "    for n, (ocr, desc, indices) in enumerate(zip(df['ocr_text'], df['triples'], df['indices'])):\n",
    "        indices = eval(indices)\n",
    "\n",
    "        print(indices)\n",
    "        #setting up the data for few shot examples:\n",
    "        _1_shot , _2_shot, _3_shot = indices[0], indices[1], indices[2]\n",
    "        _1_shot_ocr, _1_shot_desc, _1_shot_labels = train_df['ocr_text'][_1_shot], train_df['triples'][_1_shot], train_df['meme_anxiety_categories'][_1_shot]\n",
    "        # _2_shot_ocr, _2_shot_desc, _2_shot_labels = train_df['ocr_text'][_2_shot], train_df['triples'][_2_shot], train_df['meme_anxiety_categories'][_2_shot]\n",
    "        # _3_shot_ocr, _3_shot_desc, _3_shot_labels = train_df['ocr_text'][_3_shot], train_df['triples'][_3_shot], train_df['meme_anxiety_categories'][_3_shot]\n",
    "        \n",
    "        ocr = split_ocr(ocr, ocr_word_limit)\n",
    "        _1_shot_ocr = split_ocr(_1_shot_ocr, ocr_word_limit)\n",
    "        # _2_shot_ocr = split_ocr(_2_shot_ocr, ocr_word_limit)\n",
    "        # _3_shot_ocr = split_ocr(_3_shot_ocr, ocr_word_limit)\n",
    "        \n",
    "        prompt = f'''#System: You specialize in analyzing mental health behaviors through social media posts. Your task is to classify the mental health issue depicted in a person's post from the following categories: ['Difficulty Relaxing', 'Excessive Worry', 'Impending Doom', 'Irritatbily', 'Lack of Worry Control', 'Nervousness', 'Restlessness'].\n",
    "\n",
    "##Example1: \n",
    "<|ocr_text|>{_1_shot_ocr}\n",
    "<|commonsense figurative explaination|>\\n{_1_shot_desc}\n",
    "The mental health disorder of the person for this post is : {_1_shot_labels}\n",
    "\n",
    "###Your_turn:\n",
    "<|ocr_text|>{ocr}\n",
    "<|commonsense figurative explaination|>\\n{desc}\n",
    "The mental health disorder of the person for this post is : '''\n",
    "        \n",
    "        prompts.append(prompt)\n",
    "    \n",
    "    df['prompt'] = prompts\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = prompt_it(train_df)\n",
    "test_df = prompt_it(test_df)\n",
    "\n",
    "class MemeDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "        self.texts = df['prompt']\n",
    "        self.labels = df['labels']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'targets': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(list(le.classes_))\n",
    "test_df['labels'][0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df['prompt'][332])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df['labels'][332])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[train_df['sample_id'] == 'TR-1027']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mental Bart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "device_ids = [1] \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device)\n",
    "model_name = 'Tianlin668/MentalBART'\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6)\n",
    "\n",
    "# Freeze all layers except the last two layers\n",
    "for name, param in model.named_parameters():\n",
    "    if \"model.encoder.layers\" in name or \"model.decoder.layers\" in name:\n",
    "        layer_num = int(name.split('.')[3])\n",
    "        if layer_num < 10:  # Assuming BART has 12 layers\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Wrap the model with DataParallel\n",
    "model = torch.nn.DataParallel(model, device_ids=device_ids)\n",
    "model = model.to(f'cuda:{device_ids[0]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting the maximum token in the input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_tokens(df):\n",
    "  temp = pd.DataFrame(df)\n",
    "  temp['max_token_count'] = temp['prompt'].apply(lambda x: len(tokenizer.encode(x)))\n",
    "\n",
    "  # Find the maximum token count across all rows\n",
    "  max_tokens = temp['max_token_count'].max()\n",
    "\n",
    "  print(\"Maximum token count:\", max_tokens)\n",
    "  return temp['max_token_count']\n",
    "\n",
    "\n",
    "train = max_tokens(train_df)\n",
    "test = max_tokens(test_df)\n",
    "\n",
    "# Calculate and print the median\n",
    "train_median = train.median()\n",
    "train_mean = train.mean()\n",
    "test_median = test.median()\n",
    "test_mean = test.mean()\n",
    "\n",
    "print(\"Median token count (train):\", train_median)\n",
    "print(\"Mean token count (train):\", train_mean)\n",
    "\n",
    "print(\"Median token count (test):\", test_median)\n",
    "print(\"Mean token count (test):\", test_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import torch.nn.functional as F\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "# set_seed(42)\n",
    "train_dataset = MemeDataset(train_df, tokenizer, 1024)\n",
    "val_dataset = MemeDataset(test_df, tokenizer, 1024)\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training function\n",
    "def train_model(training_loader, model, optimizer, criterion):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    loop = tqdm(enumerate(training_loader), total=len(training_loader), leave=True, colour='GREEN')\n",
    "    for batch_idx, data in loop:\n",
    "        ids = data['input_ids'].to(device)\n",
    "        mask = data['attention_mask'].to(device)\n",
    "        targets = data['targets'].to(device)  # Ensure targets are class indices, not one-hot\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(ids, attention_mask=mask).logits\n",
    "        # probabilities = F.softmax(outputs, dim=1)  # Apply softmax to get probabilities\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "    return np.mean(train_losses)\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def eval_model(validation_loader, model, criterion):\n",
    "    model.eval()\n",
    "    val_targets = []\n",
    "    val_outputs = []\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(validation_loader):\n",
    "            ids = data['input_ids'].to(device)\n",
    "            mask = data['attention_mask'].to(device)\n",
    "            targets = data['targets'].to(device)  # Ensure targets are class indices\n",
    "            \n",
    "            outputs = model(ids, attention_mask=mask).logits\n",
    "            # probabilities = F.softmax(outputs, dim=1)  # Apply softmax to get probabilities\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_losses.append(loss.item())\n",
    "            \n",
    "            val_targets.extend(targets.cpu().numpy())\n",
    "            val_outputs.extend(torch.argmax(outputs, dim=1).cpu().numpy())  # Convert logits to class predictions\n",
    "\n",
    "    val_targets = np.array(val_targets)\n",
    "    val_outputs = np.array(val_outputs)\n",
    "    \n",
    "    accuracy = accuracy_score(val_targets, val_outputs)\n",
    "    \n",
    "    # Ensure that target_names and unique_classes match\n",
    "    target_names = ['Restlessness','Nervousness', 'Impending Doom', 'Difficulty Relaxing', 'Lack of Worry Control', 'Excessive Worry']\n",
    "    \n",
    "    report = classification_report(\n",
    "        val_targets, val_outputs, \n",
    "        target_names=target_names, \n",
    "        output_dict=True, \n",
    "        # labels=7\n",
    "    )\n",
    "    \n",
    "    return report, val_targets, val_outputs, np.mean(val_losses), accuracy\n",
    "\n",
    "\n",
    "# Main training loop\n",
    "EPOCHS = 10\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    print(f'Epoch {epoch}/{EPOCHS}')\n",
    "    train_loss = train_model(train_loader, model, optimizer, criterion)\n",
    "    report, val_targets, val_outputs_binary, val_loss, accuracy = eval_model(val_loader, model, criterion)\n",
    "    \n",
    "    val_f1_macro = report['macro avg']['f1-score']\n",
    "    val_f1_weighted = report['weighted avg']['f1-score']\n",
    "    print(f\"\\n||Validation Accuracy: {accuracy:.4f} |Validation F1 Macro: {val_f1_macro:.4f}| |Validation F1 Weighted: {val_f1_weighted:.4f}|\")\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_f1_weighted'].append(val_f1_weighted)\n",
    "    \n",
    "    if val_f1_macro > best_accuracy:\n",
    "        best_accuracy = val_f1_macro\n",
    "\n",
    "        print(\"Classification Report:\\n\", classification_report(\n",
    "            val_targets, val_outputs_binary,\n",
    "            target_names=['Restlessness','Nervousness', 'Impending Doom', 'Difficulty Relaxing', 'Lack of Worry Control', 'Excessive Worry']\n",
    "        ))\n",
    "\n",
    "# Plotting results\n",
    "fig, axs = plt.subplots(2, 1, figsize=(5, 7))\n",
    "\n",
    "# Plotting the training and validation losses\n",
    "axs[0].plot(history['train_loss'], label='Train Loss')\n",
    "axs[0].plot(history['val_loss'], label='Validation Loss')\n",
    "axs[0].set_title('Training and Validation Losses')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].legend()\n",
    "axs[0].grid()\n",
    "\n",
    "# Plotting the validation F1 scores\n",
    "axs[1].plot(history['val_f1_macro'], label='Validation F1 Macro')\n",
    "axs[1].plot(history['val_f1_weighted'], label='Validation F1 weighted')\n",
    "axs[1].set_title('Validation F1 Scores')\n",
    "axs[1].set_ylabel('F1 Score')\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].legend()\n",
    "axs[1].grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
